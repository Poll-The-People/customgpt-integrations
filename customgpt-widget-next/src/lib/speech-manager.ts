/**
 * Optimized Speech Manager with Unified Pipeline and HTML5 Audio
 *
 * Performance optimizations:
 * 1. Unified /inference endpoint (eliminates 3 round-trips)
 * 2. HTML5 Audio instead of Web Audio API (faster playback start)
 * 3. Progressive audio playback (starts as soon as data available)
 *
 * Expected latency: ~9.5s to first audio (vs ~20s before)
 */

import { utils } from '@ricky0123/vad-react';

let conversationHistory = btoa('[]');  // Base64 encoded conversation history
let currentAudioElement: HTMLAudioElement | null = null;

export const processSpeech = async (audio: Float32Array) => {
    const blob = createAudioBlob(audio);
    await validate(blob);
    await sendData(blob);
};

const createAudioBlob = (audio: Float32Array) => {
    const wavBuffer = utils.encodeWAV(audio);
    return new Blob([wavBuffer], { type: 'audio/wav' });
};

const validate = async (blob: Blob) => {
    const audioContext = new AudioContext({ sampleRate: 16000 });
    const arrayBuffer = await blob.arrayBuffer();
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    const duration = audioBuffer.duration;

    if (duration < 0.4) {
        throw new Error('Audio too short (possible VAD misfire)');
    }
};

const sendData = async (blob: Blob) => {
    // Check if we're in avatar mode (declare at function scope for error handler access)
    const displayMode = (window as any).getDisplayMode ? (window as any).getDisplayMode() : 'particles';
    const isAvatarMode = displayMode === 'avatar';

    try {
        const totalStart = performance.now();

        // Update state: user speaking finished ‚Üí processing
        if (isAvatarMode && (window as any).avatarSetProcessing) {
            console.log('[Speech] ü§î Avatar entering processing state');
            (window as any).avatarSetProcessing();
        } else if ((window as any).particleActions) {
            (window as any).particleActions.onProcessing();
        }

        // Use UNIFIED /inference endpoint (STT ‚Üí AI ‚Üí TTS pipeline)
        const formData = new FormData();
        formData.append('audio', blob, 'audio.wav');

        const response = await fetch('/api/inference', {
            method: 'POST',
            body: formData,
            headers: {
                'conversation': conversationHistory
            }
        });

        if (!response.ok) {
            throw new Error(`Inference failed: ${response.statusText}`);
        }

        // Extract timing and content from response headers
        const sttTime = parseFloat(response.headers.get('X-STT-Time') || '0');
        const aiTime = parseFloat(response.headers.get('X-AI-Time') || '0');
        const transcriptB64 = response.headers.get('X-Transcript') || '';
        const transcript = transcriptB64 ? atob(transcriptB64) : '';  // Decode base64 for Unicode support
        const aiResponseB64 = response.headers.get('X-AI-Response') || '';
        const aiResponse = aiResponseB64 ? atob(aiResponseB64) : '';

        // Update conversation history (server returns base64 conversation)
        conversationHistory = response.headers.get('X-Conversation') || conversationHistory;

        console.log(`[TIMING] STT: ${sttTime.toFixed(3)}s | Transcript: ${transcript}`);
        console.log(`[TIMING] AI: ${aiTime.toFixed(3)}s | Response: ${aiResponse.substring(0, 100)}...`);

        // Show user message in UI
        if ((window as any).addUserMessage) {
            (window as any).addUserMessage(transcript);
        }

        // Update particle state: AI speaking
        if ((window as any).particleActions) {
            (window as any).particleActions.onAiSpeaking();
        }

        // Play audio using HTML5 Audio for progressive playback
        const playStart = performance.now();
        await playStreamingAudio(response.body!, aiResponse);
        const playTime = performance.now() - playStart;

        const totalTime = performance.now() - totalStart;
        console.log(`[TIMING] TOTAL: ${(totalTime / 1000).toFixed(3)}s (STT: ${sttTime.toFixed(3)}s, AI: ${aiTime.toFixed(3)}s, Play: ${(playTime / 1000).toFixed(3)}s)`);

        // Reset state after audio finishes
        if (isAvatarMode && (window as any).avatarSetIdle) {
            console.log('[Speech] üòå Avatar returning to idle state');
            (window as any).avatarSetIdle();
        } else if ((window as any).particleActions) {
            (window as any).particleActions.reset();
        }

    } catch (error) {
        console.error('[ERROR] Speech processing failed:', error);

        // Reset state on error
        if (isAvatarMode && (window as any).avatarSetIdle) {
            console.log('[Speech] ‚ö†Ô∏è Avatar returning to idle after error');
            (window as any).avatarSetIdle();
        } else if ((window as any).particleActions) {
            (window as any).particleActions.reset();
        }

        throw error;
    }
};

const playStreamingAudio = async (stream: ReadableStream<Uint8Array>, aiResponse: string) => {
    // Stop any currently playing audio
    if (currentAudioElement) {
        currentAudioElement.pause();
        currentAudioElement.src = '';
        currentAudioElement = null;
    }

    // Collect all chunks (required for MP3 decoding)
    const chunks: any[] = [];
    const reader = stream.getReader();

    try {
        while (true) {
            const { done, value } = await reader.read();

            if (done) {
                break;
            }

            chunks.push(value as BlobPart);
        }
    } finally {
        reader.releaseLock();
    }

    // Combine chunks into single blob
    const audioBlob = new Blob(chunks, { type: 'audio/mpeg' });
    const audioUrl = URL.createObjectURL(audioBlob);

    // Check if we're in avatar mode
    const displayMode = (window as any).getDisplayMode ? (window as any).getDisplayMode() : 'particles';
    const isAvatarMode = displayMode === 'avatar';

    console.log('[Speech] Display mode:', displayMode, '| Avatar mode:', isAvatarMode);

    // Update UI with AI response and audio URL (for avatar mode)
    if ((window as any).updateCaptions) {
        (window as any).updateCaptions(aiResponse, audioUrl);
    }

    // In avatar mode, TalkingHead will handle audio playback for lip-sync
    // In particle mode, play through HTML5 Audio element
    if (isAvatarMode) {
        console.log('[Speech] Avatar mode - TalkingHead will handle audio playback');

        // Wait for avatar to finish speaking
        if ((window as any).waitForAvatarSpeak) {
            await (window as any).waitForAvatarSpeak();
            console.log('[Speech] Avatar speaking complete');
        } else {
            console.warn('[Speech] waitForAvatarSpeak not available, waiting 5s fallback');
            await new Promise(resolve => setTimeout(resolve, 5000));
        }

        // Don't revoke immediately - wait for avatar speak complete callback
        // The blob URL will be cleaned up after TalkingHead has fetched the audio
    } else {
        console.log('[Speech] Particle mode - playing audio via HTML5 Audio element');

        // Create HTML5 Audio element for playback
        currentAudioElement = new Audio(audioUrl);

        // Wait for audio to be playable
        await new Promise<void>((resolve, reject) => {
            if (!currentAudioElement) {
                reject(new Error('Audio element not created'));
                return;
            }

            currentAudioElement.oncanplaythrough = () => resolve();
            currentAudioElement.onerror = () => reject(new Error('Audio playback failed'));

            // Start loading
            currentAudioElement.load();
        });

        // Play audio
        await currentAudioElement.play();

        // Wait for playback to complete
        await new Promise<void>((resolve) => {
            if (!currentAudioElement) {
                resolve();
                return;
            }

            currentAudioElement.onended = () => {
                // Cleanup
                URL.revokeObjectURL(audioUrl);
                resolve();
            };
        });
    }
};

// Global stop function for UI stop button
(window as any).stopAudio = () => {
    if (currentAudioElement) {
        currentAudioElement.pause();
        currentAudioElement.src = '';
        currentAudioElement = null;
    }

    if ((window as any).particleActions) {
        (window as any).particleActions.reset();
    }
};

// Export functions for VAD integration
export const onSpeechStart = () => {
    console.log('[VAD] Speech started');

    // Check if we're in avatar mode
    const displayMode = (window as any).getDisplayMode ? (window as any).getDisplayMode() : 'particles';
    const isAvatarMode = displayMode === 'avatar';

    if (isAvatarMode && (window as any).avatarSetListening) {
        console.log('[VAD] üëÇ Avatar entering listening state (user speaking)');
        (window as any).avatarSetListening();
    } else if ((window as any).particleActions) {
        (window as any).particleActions.onUserSpeaking();
    }
};

export const onSpeechEnd = processSpeech;

export const onMisfire = () => {
    console.log('[VAD] Misfire detected');

    // Check if we're in avatar mode
    const displayMode = (window as any).getDisplayMode ? (window as any).getDisplayMode() : 'particles';
    const isAvatarMode = displayMode === 'avatar';

    if (isAvatarMode && (window as any).avatarSetIdle) {
        console.log('[VAD] ‚ö†Ô∏è Avatar returning to idle after misfire');
        (window as any).avatarSetIdle();
    } else if ((window as any).particleActions) {
        (window as any).particleActions.reset();
    }
};
